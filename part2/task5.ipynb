{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "yp5Nn7lzV3J6"
      },
      "source": [
        "# Load all imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "sZzTc_qcr0ge"
      },
      "outputs": [],
      "source": [
        "import torch\r\n",
        "import torchvision\r\n",
        "import torchvision.transforms as transforms\r\n",
        "import numpy as np\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import helper\r\n",
        "import net"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Globals, CNNs, and Device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda:0\n",
            "951274\n",
            "983978\n"
          ]
        }
      ],
      "source": [
        "oriPATH = 'C:/Users/Ryan/Desktop/machine-learning/part2/cnn'\r\n",
        "classes = ('0', '1', '2', '3', '4', '5', '6', '7', '8', '9')\r\n",
        "learningRates = [0.00001, 0.0001, 0.001, 0.01, 0.1, 1]\r\n",
        "numEpochs = 50\r\n",
        "\r\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\r\n",
        "print(device)\r\n",
        "\r\n",
        "wideNet = net.ReluNetWide()\r\n",
        "deepNet = net.ReluNetDeep()\r\n",
        "print(sum([p.numel() for p in wideNet.parameters()]))\r\n",
        "print(sum([p.numel() for p in deepNet.parameters()]))\r\n",
        "wideNet = wideNet.to(device)\r\n",
        "deepNet = deepNet.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The wide and deep nets both have similar numbers of values and are approximately the same size as the original network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "m77sHXmJWHfx"
      },
      "source": [
        "# MNIST training and validation set with augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "JNDByYIhr4TU"
      },
      "outputs": [],
      "source": [
        "valid_ratio = 0.3\r\n",
        "\r\n",
        "transform = transforms.Compose([\r\n",
        "    # define your data deepmentation here!\r\n",
        "    transforms.RandomRotation(degrees=60),\r\n",
        "    transforms.RandomRotation(degrees=300),\r\n",
        "    transforms.RandomRotation(degrees=30),\r\n",
        "    transforms.RandomRotation(degrees=330),\r\n",
        "    transforms.ToTensor(),\r\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\r\n",
        "])\r\n",
        "\r\n",
        "transformed = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\r\n",
        "\r\n",
        "nb_train = int((1.0 - valid_ratio) * len(transformed))\r\n",
        "nb_valid =  int(valid_ratio * len(transformed))\r\n",
        "t_train_dataset, t_valid_dataset = torch.utils.data.dataset.random_split(transformed, [nb_train, nb_valid])\r\n",
        "t_trainloader = torch.utils.data.DataLoader(t_train_dataset, batch_size=1000, shuffle=True, num_workers=2, pin_memory=True)\r\n",
        "t_validloader = torch.utils.data.DataLoader(t_valid_dataset, batch_size=1000, shuffle=True, num_workers=2, pin_memory=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "slDGVTtkXd-Z"
      },
      "source": [
        "# Define the loss function and the optimizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "ml4xvTi7sgCE"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\r\n",
        "import torch.optim as optim\r\n",
        "\r\n",
        "criterion = nn.CrossEntropyLoss()\r\n",
        "\r\n",
        "wideOpt = optim.Adam(wideNet.parameters(), lr=0.001)\r\n",
        "deepOpt = optim.Adam(deepNet.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8dLwz9XsYkNJ"
      },
      "source": [
        "# Train the CNN and store the best model based on the validation loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "colab_type": "code",
        "id": "csCvcF7Ss1Ud",
        "outputId": "7e62b4de-04c9-4cb8-d70b-aaeb5071f410"
      },
      "outputs": [],
      "source": [
        "import time\r\n",
        "import os as OO\r\n",
        "OO.mkdir(oriPATH + '/task5')\r\n",
        "PATH = oriPATH + '/task5'\r\n",
        "\r\n",
        "wideTrainingLoss = []\r\n",
        "wideValidationLoss = []\r\n",
        "deepTrainingLoss = []\r\n",
        "deepValidationLoss = []\r\n",
        "\r\n",
        "\r\n",
        "train, val = helper.runCNN_earlyStop(t_trainloader, device, deepOpt, deepNet, criterion, t_validloader, PATH, 'deepNet', numEpochs)\r\n",
        "deepTrainingLoss.append(train)\r\n",
        "deepValidationLoss.append(val)\r\n",
        "train, val = helper.runCNN_earlyStop(t_trainloader, device, wideOpt, wideNet, criterion, t_validloader, PATH, 'wideNet', numEpochs)\r\n",
        "wideTrainingLoss.append(train)\r\n",
        "wideValidationLoss.append(val)\r\n",
        "\r\n",
        "np.save(OO.path.join(PATH, 'wideTrainingLoss.npy'), wideTrainingLoss)\r\n",
        "np.save(OO.path.join(PATH, 'wideValidationLoss.npy'), wideValidationLoss)\r\n",
        "np.save(OO.path.join(PATH, 'deepTrainingLoss.npy'), deepTrainingLoss)\r\n",
        "np.save(OO.path.join(PATH, 'deepValidationLoss.npy'), deepValidationLoss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "yfvPe-jSYsrR"
      },
      "source": [
        "# Define the test dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "UQSOIHv7yf-3"
      },
      "outputs": [],
      "source": [
        "transform = transforms.Compose([\r\n",
        "     transforms.ToTensor(),\r\n",
        "     transforms.Normalize((0.1307,), (0.3081,))\r\n",
        "])\r\n",
        "testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3r7FPw9MZoMB"
      },
      "source": [
        "# Infer on the whole test dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "colab_type": "code",
        "id": "F246Hc0QzLLV",
        "outputId": "95e3f967-1334-4738-edd6-535cccdf2486"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "wideular network: 48 epochs 97.500 % accuracy\n",
            "deep stop network: 49 epochs 97.500 % accuracy\n"
          ]
        }
      ],
      "source": [
        "testloader = torch.utils.data.DataLoader(testset, batch_size=1000, shuffle=False, num_workers=2, pin_memory=True)\r\n",
        "\r\n",
        "import os as OO\r\n",
        "PATH = oriPATH + '/task5'\r\n",
        "\r\n",
        "accuracy = []\r\n",
        "\r\n",
        "testNet = net.ReluNetWide()\r\n",
        "testNet = testNet.to(device)\r\n",
        "testNet.load_state_dict(torch.load(OO.path.join(PATH, 'wideNet.pth')))\r\n",
        "correct, total = helper.testCNN(testloader, testNet, device)\r\n",
        "accuracy.append(100 * correct / total)\r\n",
        "testNet2 = net.ReluNetDeep()\r\n",
        "testNet2 = testNet2.to(device)\r\n",
        "testNet2.load_state_dict(torch.load(OO.path.join(PATH, 'deepNet.pth')))\r\n",
        "correct, total = helper.testCNN(testloader, testNet, device)\r\n",
        "accuracy.append(100 * correct / total)\r\n",
        "\r\n",
        "sTrain = np.load(OO.path.join(PATH, 'wideTrainingLoss.npy'))\r\n",
        "rTrain = np.load(OO.path.join(PATH, 'deepTrainingLoss.npy'))\r\n",
        "\r\n",
        "wideNumEpochs = len(sTrain[0])\r\n",
        "deepNumEpochs = len(rTrain[0])\r\n",
        "\r\n",
        "print(\"wide network: {} epochs %.3F %% accuracy\".format(wideNumEpochs) % accuracy[0])\r\n",
        "print(\"deep stop network: {} epochs %.3F %% accuracy\".format(deepNumEpochs) % accuracy[1])\r\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "demo.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}